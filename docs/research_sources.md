# Research Sources

## vLLM
- "State-of-the-art serving throughput" and "Efficient management of attention key and value memory with PagedAttention" — [vLLM README](https://raw.githubusercontent.com/vllm-project/vllm/main/README.md) (accessed 16 Aug 2025).
- "OpenAI-compatible API server" and "Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron" — [vLLM README](https://raw.githubusercontent.com/vllm-project/vllm/main/README.md) (accessed 16 Aug 2025).

## llama.cpp
- "Launch OpenAI-compatible API server" — [llama.cpp README](https://raw.githubusercontent.com/ggerganov/llama.cpp/master/README.md) (accessed 16 Aug 2025).
- "Plain C/C++ implementation without any dependencies" and "CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity" — [llama.cpp README](https://raw.githubusercontent.com/ggerganov/llama.cpp/master/README.md) (accessed 16 Aug 2025).

## Ollama
- "Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models" — [Ollama README](https://raw.githubusercontent.com/ollama/ollama/main/README.md) (accessed 16 Aug 2025).
- "`100% GPU` means the model was loaded entirely into the GPU", "`100% CPU` means the model was loaded entirely in system memory", "`48%/52% CPU/GPU` means the model was loaded partially onto both the GPU and into system memory" — [Ollama FAQ](https://raw.githubusercontent.com/ollama/ollama/main/docs/faq.md) (accessed 16 Aug 2025).
- "POST /api/chat" ... "Generate the next message in a chat with a provided model. This is a streaming endpoint" — [Ollama API docs](https://raw.githubusercontent.com/ollama/ollama/main/docs/api.md) (accessed 16 Aug 2025).
